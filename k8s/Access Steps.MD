# EKS Bastion Access Steps

**Steps to connect the Bastion host to the Private EKS Cluster after any `terraform apply`.**
*(All commands are executed from within the Bastion host)*

## Cluster Information
- **Cluster Name:** `flower-shop-cluster`
- **Region:** `us-east-1`
- **Bastion Role Name:** `bastion-role`

---

## Step 1: Bootstrap Login as IAM Admin
**Goal:** This step is required only once to update the `aws-auth` ConfigMap and add the `bastion-role` to the cluster. This is done after the cluster is created by Terraform.

1. **Configure AWS CLI on Bastion:**
   ```bash
   aws configure
   ```
   *Enter the AWS Access Key / Secret for the admin user (e.g., `terraform` user).*

2. **Verify Identity:**
   ```bash
   aws sts get-caller-identity
   ```
   *Ensure the ARN shows `user/terraform` and NOT `assumed-role/bastion-role`.*

---

## Step 2: Update Kubeconfig as Admin
**Goal:** Generate the kubeconfig file using the admin credentials to gain initial access.

1. **Update Kubeconfig:**
   ```bash
   aws eks update-kubeconfig --name flower-shop-cluster --region us-east-1
   ```

   > **Note:** If you see "The connection to the server localhost:8080 was refused", it is normal at this stage if `kubectl` cannot find a valid config. It should resolve after this step.






### Troubleshooting: "Connection Refused" Error

If you encounter the following error:
> "The connection to the server localhost:8080 was refused"

**Cause:** This is normal if `kubectl` cannot find a valid kubeconfig file. It defaults to trying `localhost:8080`, which does not exist.

**Solution:**
Run the following commands to fix permissions and update the kubeconfig. After this, the error should disappear. ðŸŸ¢

```bash
sudo mkdir -p /home/ubuntu/.kube
sudo chown -R ubuntu:ubuntu /home/ubuntu/.kube
aws eks update-kubeconfig --name flower-shop-cluster --region us-east-1
```


2. **Verify Access:**
   ```bash
   kubectl get nodes
   ```
   *It is normal to see "No resources found" if Fargate nodes are not yet active.*

---

## Step 3: Edit `aws-auth` to Add `bastion-role`
**Goal:** Grant `system:masters` (admin) permissions to any token generated by the `bastion-role`.

1. **Edit ConfigMap:**
   ```bash
   kubectl edit configmap aws-auth -n kube-system
   ```

2. **Add the Role Mapping:**
   Inside the editor, look for `mapRoles: |`. Add the following block under the existing roles (maintain indentation):

   ```yaml
   - rolearn: arn:aws:iam::163511166008:role/bastion-role
     username: bastion-admin
     groups:
       - system:masters
   ```

3. **Save and Exit:**
   - If using Vim: Press `Esc`, type `:wq`, and hit `Enter`.

---

## Step 4: Remove Admin Credentials
**Goal:** Switch to using the EC2 instance profile (`bastion-role`) exclusively for security.

1. **Remove Credentials:**
   ```bash
   rm -f ~/.aws/credentials
   rm -f ~/.aws/config
   ```

2. **Verify Identity:**
   ```bash
   aws sts get-caller-identity
   ```
   *The ARN should now look like: `arn:aws:sts::...:assumed-role/bastion-role/...`*

---

## Step 5: Update Kubeconfig as `bastion-role`
**Goal:** Refresh the kubeconfig to use the instance profile and verify final access.

1. **Update Kubeconfig:**
   ```bash
   aws eks update-kubeconfig --name flower-shop-cluster --region us-east-1
   ```

2. **Verify Access:**
   ```bash
   kubectl get nodes
   kubectl get pods -A
   ```
   *If these commands run without error, the Bastion host is successfully configured as a Cluster Admin.*

---

## Troubleshooting: CoreDNS on Fargate

If your pods are stuck in `Pending` state, it might be due to CoreDNS not being scheduled on Fargate yet.

### 1. Check CoreDNS Pods
```bash
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
```

### 2. Restart CoreDNS
If they are pending, delete them to force a restart on Fargate nodes:
```bash
kubectl delete pod -n kube-system -l k8s-app=kube-dns
```

### 3. Verify Resolution
After a few minutes, check again:
```bash
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
```
*You should see them in `Running` state with valid IPs.*

### 4. Verify Fargate Nodes
```bash
kubectl get nodes
```
*You should see nodes like `fargate-ip-10-0-x-x.ec2.internal` in `Ready` state.*

---

## Summary of Workflow
Whenever you run `terraform destroy` and then `terraform apply`:
1. Wait for the cluster to be created.
2. SSH into the Bastion.
3. Repeat **Steps 1 through 5**.
   - *Note:* The `bastion-role` ARN usually remains constant unless renamed in Terraform, but Fargate roles may change, so let Terraform handle those.
